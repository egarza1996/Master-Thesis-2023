{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b527d30-7c9b-4d09-afab-4d6a690583fb",
   "metadata": {},
   "source": [
    "# Clustering \n",
    "\n",
    "(Run this code after you have run the matlab code \"clustering_meta_k_means.m\")\n",
    "\n",
    "### 1) Spatial distribution of neuron clusters\n",
    "### 2) Correlation coefficients between cluster centroids\n",
    "### 3) Spatial correlations between cluster centroids\n",
    "### 4) Correlations between individual neurons' activity patterns\n",
    "### 5) Spectral Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbb6d2d-8aa5-4724-8510-10231b05190f",
   "metadata": {},
   "source": [
    "## Setting up environment and defining paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "9077396f-8506-451c-92cb-4ad6c065b9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1453689/82211235.py:34: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  get_ipython().magic('load_ext autoreload')\n",
      "/tmp/ipykernel_1453689/82211235.py:35: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  get_ipython().magic('autoreload 2')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"2859\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  const force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "const JS_MIME_TYPE = 'application/javascript';\n",
       "  const HTML_MIME_TYPE = 'text/html';\n",
       "  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  const CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    const script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    const cell = handle.cell;\n",
       "\n",
       "    const id = cell.output_area._bokeh_element_id;\n",
       "    const server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd_clean, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            const id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd_destroy);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    const output_area = handle.output_area;\n",
       "    const output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      const bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      const script_attrs = bk_div.children[0].attributes;\n",
       "      for (let i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      const toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    const events = require('base/js/events');\n",
       "    const OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  const NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    const el = document.getElementById(\"2859\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error(url) {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < css_urls.length; i++) {\n",
       "      const url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < js_urls.length; i++) {\n",
       "      const url = js_urls[i];\n",
       "      const element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.3.min.js\", \"https://unpkg.com/@holoviz/panel@0.14.4/dist/panel.min.js\"];\n",
       "  const css_urls = [];\n",
       "\n",
       "  const inline_js = [    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "function(Bokeh) {\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "          for (let i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      const cell = $(document.getElementById(\"2859\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    const el = document.getElementById(\"2859\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.3.min.js\", \"https://unpkg.com/@holoviz/panel@0.14.4/dist/panel.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n          for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\nif (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"2859\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import h5py\n",
    "import scipy.sparse as sp\n",
    "from scipy.io import loadmat\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import pearsonr, iqr\n",
    "from scipy.signal import welch\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.lines as mlines\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pywt\n",
    "\n",
    "import bokeh.plotting as bpl\n",
    "import cv2\n",
    "import glob\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "try:\n",
    "    cv2.setNumThreads(0)\n",
    "except():\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    if __IPYTHON__:\n",
    "        # this is used for debugging purposes only. allows to reload classes\n",
    "        # when changed\n",
    "        get_ipython().magic('load_ext autoreload')\n",
    "        get_ipython().magic('autoreload 2')\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "import caiman as cm\n",
    "from caiman.motion_correction import MotionCorrect\n",
    "from caiman.source_extraction.cnmf import cnmf as cnmf\n",
    "from caiman.source_extraction.cnmf import params as params\n",
    "from caiman.utils.utils import download_demo\n",
    "from caiman.utils.visualization import plot_contours, nb_view_patches, nb_plot_contour\n",
    "bpl.output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5978cae9-c225-4758-a4bf-50247459c943",
   "metadata": {},
   "source": [
    "### Only things you need to define are: organoid_number, run, and save_figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "c02fdc1d-c468-44fb-bbd4-79db37c2016b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your variables\n",
    "organoid_number = 'organoid1'\n",
    "run = 'run_2'\n",
    "\n",
    "# Define 1 to save the figure, 0 otherwise\n",
    "save_figure = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "f6aede35-10c9-4b34-87fb-73437ab97a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to Analysis Results: /home/silviu/erika-organoid-data/CODE/CURRENT-FOLDER/LOOP/exp1-2_11-25_07_23/organoid1/run_2\n",
      "Path to Clustering Data: /home/silviu/erika-organoid-data/CODE/CURRENT-FOLDER/Clustering-Correlation-Distance/loop-new/organoid1/run_2\n"
     ]
    }
   ],
   "source": [
    "# Construct your paths using string formatting\n",
    "base_path1 = '/home/silviu/erika-organoid-data/CODE/CURRENT-FOLDER/LOOP/exp1-2_11-25_07_23'\n",
    "base_path2 = '/home/silviu/erika-organoid-data/CODE/CURRENT-FOLDER/Clustering-Correlation-Distance/loop-new'\n",
    "\n",
    "path_analysis_results = f'{base_path1}/{organoid_number}/{run}' # Contains the \"analysis_results.hdf5\" file which is the ouput of CaIman\n",
    "path_clustering_data = f'{base_path2}/{organoid_number}/{run}' # Contains the \"clustering_data.mat\" file which is the output of the \"meta_k_means\" code\n",
    "\n",
    "# Now, path_analysis_results and path_clustering_data are constructed from the variables\n",
    "print(\"Path to Analysis Results:\", path_analysis_results)\n",
    "print(\"Path to Clustering Data:\", path_clustering_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "eb9066f6-2798-4254-8d62-88f8857bb831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For manually setting the paths\n",
    "# path_analysis_results = '/home/silviu/erika-organoid-data/CODE/CURRENT-FOLDER/LOOP/exp1-2_11-25_07_23/organoid4/run'\n",
    "# path_clustering_data = '/home/silviu/erika-organoid-data/CODE/CURRENT-FOLDER/Clustering-Correlation-Distance/loop-new/organoid4/run'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "053d0fc5-d03e-427a-b2a7-aaf27fe2149a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion ratios from pixels to MICROMETERS for different organoids\n",
    "# Depends on the microscope magnification used for each organoid's recordings\n",
    "conversion_ratios = {'organoid7': 10.6,   # 94 pixels = 1 mm\n",
    "                     'organoid1': 6.58,   # 152 pixels = 1 mm\n",
    "                     'organoid2': 6.58,   # 152 pixels = 1 mm\n",
    "                     'organoid6': 6.58,   # 152 pixels = 1 mm\n",
    "                     'organoid5': 5.26,   # 190 pixels = 1 mm\n",
    "                     'organoid3': 4.08,   # 245 pixels = 1 mm\n",
    "                     'organoid4': 4.08,   # 245 pixels = 1 mm\n",
    "                     'organoid8': 4.08,   # 245 pixels = 1 mm\n",
    "                     'organoid9': 4.08}   # 245 pixels = 1 mm\n",
    "\n",
    "# Determine the conversion factor based on the organoid_number\n",
    "conversion_factor = conversion_ratios.get(organoid_number, None)\n",
    "if conversion_factor is None:\n",
    "    raise ValueError(f\"No conversion ratio defined for organoid number {organoid_number}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81038ab3-fdb9-4824-a1fc-2a21a818a368",
   "metadata": {},
   "source": [
    "## 1) Spatial distribution of neuron clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8252a2-d26c-437d-8063-0e0cea5979eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decode mmap filename /home/silviu/erika-organoid-data/CODE/CURRENT-FOLDER/LOOP/exp1-2_11-25_07_23/organoid1/run_2/memmap__d1_540_d2_640_d3_1_order_C_frames_12714.mmap\n"
     ]
    }
   ],
   "source": [
    "# Automatically find the 'analysis_results.hdf5' and the largest '.mmap' file in the specified directory\n",
    "analysis_file = os.path.join(path_analysis_results, 'analysis_results.hdf5')\n",
    "\n",
    "mmap_files = [f for f in os.listdir(path_analysis_results) if f.endswith('.mmap')]\n",
    "if not mmap_files:\n",
    "    raise FileNotFoundError(\"No .mmap files found in the specified directory.\")\n",
    "largest_mmap_file = max(mmap_files, key=lambda f: os.path.getsize(os.path.join(path_analysis_results, f)))\n",
    "mmap_file_path = os.path.join(path_analysis_results, largest_mmap_file)\n",
    "\n",
    "# Load the data from the HDF5 file\n",
    "with h5py.File(analysis_file, 'r') as f:\n",
    "    estimates = f['estimates']\n",
    "    data = estimates['A']['data'][:]\n",
    "    indices = estimates['A']['indices'][:]\n",
    "    indptr = estimates['A']['indptr'][:]\n",
    "    shape = estimates['A']['shape'][:]\n",
    "    A = sp.csc_matrix((data, indices, indptr), shape=shape)\n",
    "    C = np.array(estimates['C'])\n",
    "    dims = np.array(estimates['dims'])\n",
    "\n",
    "# Load the memory-mapped file to calculate the correlation image\n",
    "Yr, dims, T = cm.load_memmap(mmap_file_path)\n",
    "images = np.reshape(Yr.T, [T] + list(dims), order='F')\n",
    "Cn = cm.local_correlations(images.transpose(1, 2, 0))\n",
    "Cn[np.isnan(Cn)] = 0\n",
    "\n",
    "# Create a CNMF object\n",
    "cnm = cnmf.CNMF(n_processes=None, dview=None)\n",
    "\n",
    "# Create an Estimates object and populate with loaded results\n",
    "cnm.estimates = cnmf.Estimates()\n",
    "cnm.estimates.A = A\n",
    "cnm.estimates.C = C\n",
    "cnm.estimates.dims = dims\n",
    "\n",
    "# Automatic contrast adjustment using Histogram Equalization\n",
    "Cn = cv2.normalize(Cn, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "Cn = cv2.equalizeHist((Cn * 255).astype(np.uint8)).astype(np.float32) / 255\n",
    "\n",
    "# Use native CaImAn functions for plotting\n",
    "cnm.estimates.plot_contours_nb(img=Cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa7268a-ad7c-4416-8fb8-62181f9ffb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the centroids of the neurons\n",
    "centroids = np.zeros((A.shape[1], 2))  # Assuming A is Neurons x Pixels\n",
    "for i, a in enumerate(A.T):\n",
    "    spatial_footprint = a.toarray().reshape(*dims, order='F')\n",
    "    y, x = np.indices(dims)\n",
    "    total_intensity = spatial_footprint.sum()\n",
    "    centroids[i, 0] = (x * spatial_footprint).sum() / total_intensity  # x-coordinate (column index)\n",
    "    centroids[i, 1] = (y * spatial_footprint).sum() / total_intensity  # y-coordinate (row index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12afc416-baad-4411-a14e-bb29edfd0be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c1ee5f-ba17-4eba-a638-5437bab4d354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 'clustering_data.mat' from the specified directory\n",
    "# matlab_data_path = os.path.join(path_clustering_data, 'clustering_data.mat')\n",
    "matlab_data_path = os.path.join(path_clustering_data, 'clustering_data_new.mat') # trying with the \"new\"\n",
    "matlab_data = loadmat(matlab_data_path)\n",
    "\n",
    "allclusters_new = matlab_data['allclusters_new']\n",
    "# allclusters = matlab_data['allclusters'] # Contains the neuron indexes that correspond to clusters (this is a cell array containing the final clusters (with the neuron indexes that correspond to it) and their centroids)\n",
    "remove_columns = matlab_data['remove_columns'] # Contains 1's for the columns that got deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e671f6a1-4728-43a1-8cfa-d0689b07a13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('/home/silviu/Downloads/matlab_data.npy', matlab_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4732aa5a-6956-4170-9766-3ec26ff37db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the length of the 'remove_columns' matches the number of entries in the centroids array to ensure they are in sync\n",
    "if centroids.shape[0] != remove_columns.shape[1]:\n",
    "    raise ValueError(\"Length of centroids and remove_columns do not match!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d3e4ff-c22e-4ede-8b5b-30c69341e858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the 'remove_columns' variable to filter out entries from the centroids array\n",
    "filtered_centroids = centroids[remove_columns.flatten() == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3148311-c4c1-48a8-8451-714227661e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('/home/silviu/Downloads/filtered_centroids.npy', filtered_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1cee67-aba1-49b3-a478-92a4fa9c9425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and process the allclusters matrix\n",
    "neuron_indices = [row[0].flatten() for row in matlab_data['allclusters_new'][:, 0]]\n",
    "cluster_ids = list(range(1, len(neuron_indices) + 1))\n",
    "\n",
    "neuron_to_cluster = {}\n",
    "for cluster_id, indices in zip(cluster_ids, neuron_indices):\n",
    "    for idx in indices:\n",
    "        # Only assign the neuron to a cluster if it hasn't been assigned yet\n",
    "        if idx not in neuron_to_cluster:\n",
    "            neuron_to_cluster[idx] = cluster_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98334b6-057d-4bec-80dd-ce731a99a510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neuron_to_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65018f7a-9ea5-4e9a-82f0-2205af4b85d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate cluster assignments for each centroid based on the neuron_to_cluster mapping\n",
    "cluster_assignments = np.array([neuron_to_cluster.get(i+1, 0) for i in range(len(filtered_centroids))])\n",
    "\n",
    "# Stack centroids with their corresponding cluster assignments\n",
    "labeled_neurons = np.column_stack((filtered_centroids, cluster_assignments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7110ac-7173-4636-a6f3-fe0040003a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labeled_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fbd900-6d8a-44ab-91f6-8721194dab21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('/home/silviu/Downloads/labeled_neurons.npy', labeled_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbbc0ab-57b0-4756-9f23-2d9b0ce3d18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique cluster indices\n",
    "unique_clusters = np.unique(labeled_neurons[:, 2])\n",
    "\n",
    "# Define a custom colormap\n",
    "darker_grey = (0.3, 0.3, 0.3)\n",
    "colors = [darker_grey] + [plt.cm.jet(i) for i in range(plt.cm.jet.N)]\n",
    "custom_colormap = mcolors.LinearSegmentedColormap.from_list('custom_colormap', colors, N=len(colors))\n",
    "\n",
    "# White edge around the grey colored cluster \"not assigned\"\n",
    "colors_array = [custom_colormap(cluster / (len(unique_clusters) - 1 if cluster != 0 else 1))\n",
    "                for cluster in labeled_neurons[:, 2]]\n",
    "# Only white edges:\n",
    "# edge_colors = ['white' if cluster == 0 else color for cluster, color in zip(labeled_neurons[:, 2], colors_array)]\n",
    "# White and black edges around the specified clusters:\n",
    "edge_colors = ['white' if cluster == 0 else 'black' for cluster in labeled_neurons[:, 2]]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Display the flipped background image\n",
    "flipped_Cn = np.flipud(Cn)\n",
    "plt.imshow(flipped_Cn, cmap='gray', extent=[0, dims[1], dims[0], 0])  # extent sets the limits of the image in the plot\n",
    "\n",
    "# Extract original coordinates\n",
    "original_x = labeled_neurons[:, 1]\n",
    "original_y = labeled_neurons[:, 0]\n",
    "\n",
    "# Apply cumulative transformations:\n",
    "# Two 90 degree clockwise rotations and two mirrorings over X-axis\n",
    "final_x = dims[0] - original_x  # Mirroring over X-axis after rotations\n",
    "final_y = original_y  # Y-coordinates remain the same after rotations and mirroring over X-axis\n",
    "\n",
    "# Overlay the final scatter plot\n",
    "# Without white edges, just make sure to comment out the code further above that does that...\n",
    "# scatter = plt.scatter(final_y, final_x, c=labeled_neurons[:, 2], cmap=custom_colormap, s=100, alpha=0.6)\n",
    "# With white edges, but no black edges around the nuerons that were clustered\n",
    "# scatter = plt.scatter(final_y, final_x, c=labeled_neurons[:, 2], cmap=custom_colormap, s=100, alpha=0.6, edgecolor=edge_colors)\n",
    "scatter = plt.scatter(final_y, final_x, c=labeled_neurons[:, 2], cmap=custom_colormap, s=100, alpha=0.6, edgecolor=edge_colors) \n",
    "\n",
    "\n",
    "# Create legend\n",
    "adjusted_labels = ['Not Assigned' if cluster == 0 else f'Cluster {int(cluster)}' for cluster in unique_clusters]\n",
    "legend_patches = [mpatches.Patch(color=custom_colormap(cluster / (len(unique_clusters) - 1 if cluster != 0 else 1)), label=label) \n",
    "                  for cluster, label in sorted(zip(unique_clusters, adjusted_labels))]\n",
    "plt.legend(handles=legend_patches, loc='upper left')\n",
    "\n",
    "plt.title('Spatial Distribution of Neuron Clusters')\n",
    "plt.xlabel('X Coordinate')\n",
    "plt.ylabel('Y Coordinate')\n",
    "\n",
    "# Adjust the axis limits\n",
    "plt.xlim(0, 640)\n",
    "plt.ylim(0, 540)\n",
    "\n",
    "# Save the figure\n",
    "if save_figure:\n",
    "    plt.savefig(os.path.join(path_clustering_data, 'Spatial-Dist-Neuron-Clusters-new.png'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1859fa45-f442-473a-b4bc-55f6486f95b8",
   "metadata": {},
   "source": [
    "#### Rarely happens but if the plot above messes up the legend of color assignments, try this code:\n",
    "Sometimes happens for cases with datasets where ALL neurons were assigned to a cluster.\n",
    "The usual case is that there are one or two neurons that do not get assigned to a cluster."
   ]
  },
  {
   "cell_type": "raw",
   "id": "3c00a8b9-e5c3-45a8-ac65-bbd28f3f3170",
   "metadata": {},
   "source": [
    "dims = Cn.shape  # Assuming dims is the dimension of your image\n",
    "\n",
    "# Define the adjusted custom colormap\n",
    "darker_grey = (0.3, 0.3, 0.3)  # For \"Not Assigned\"\n",
    "base_colors = [plt.cm.jet(i) for i in range(plt.cm.jet.N)]  # Base colors from plt.cm.jet\n",
    "custom_colormap_adjusted = mcolors.LinearSegmentedColormap.from_list('custom_colormap', base_colors, N=len(base_colors))\n",
    "\n",
    "# Get the unique cluster indices\n",
    "unique_clusters = np.unique(labeled_neurons[:, 2])\n",
    "\n",
    "# Adjust the color assignment logic\n",
    "colors_array_adjusted = [custom_colormap_adjusted((cluster-1) / (len(unique_clusters)-1)) for cluster in labeled_neurons[:, 2]]\n",
    "edge_colors_adjusted = ['white' if cluster == 0 else 'black' for cluster in labeled_neurons[:, 2]]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 8))\n",
    "flipped_Cn = np.flipud(Cn)\n",
    "plt.imshow(flipped_Cn, cmap='gray', extent=[0, dims[1], dims[0], 0])\n",
    "\n",
    "original_x = labeled_neurons[:, 1]\n",
    "original_y = labeled_neurons[:, 0]\n",
    "final_x = dims[0] - original_x\n",
    "final_y = original_y\n",
    "\n",
    "scatter = plt.scatter(final_y, final_x, c=labeled_neurons[:, 2], cmap=custom_colormap_adjusted, s=100, alpha=0.6, edgecolor=edge_colors_adjusted)\n",
    "\n",
    "# Adjust the legend\n",
    "legend_patches_adjusted = []\n",
    "for cluster, label in sorted(zip(unique_clusters, adjusted_labels)):\n",
    "    if cluster == 0:  # For \"Not Assigned\"\n",
    "        color = darker_grey\n",
    "    else:\n",
    "        color = custom_colormap_adjusted((cluster-1) / (len(unique_clusters)-1))\n",
    "    patch = mpatches.Patch(color=color, label=label)\n",
    "    legend_patches_adjusted.append(patch)\n",
    "\n",
    "plt.legend(handles=legend_patches_adjusted, loc='upper left')\n",
    "\n",
    "plt.title('Spatial Distribution of Neuron Clusters')\n",
    "plt.xlabel('X Coordinate')\n",
    "plt.ylabel('Y Coordinate')\n",
    "plt.xlim(0, 640)\n",
    "plt.ylim(0, 540)\n",
    "\n",
    "# Save the figure\n",
    "if save_figure:\n",
    "    plt.savefig(os.path.join(path_clustering_data, 'Spatial-Dist-Neuron-Clusters-colorfix.png'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b288bb-ec3d-46e7-9245-3c44bf1b2923",
   "metadata": {},
   "source": [
    "## 2) Correlation coefficients between cluster centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e32fe4-77a3-474c-a69b-f1b24344fae1",
   "metadata": {},
   "source": [
    "This code generates a heatmap to visualize the pairwise correlation coefficients between the centroids of clusters obtained from your data. The correlation coefficient provides a measure of how similar the activity patterns of two centroids are, with values close to 1 indicating strong similarity and values close to -1 indicating dissimilarity.\n",
    "\n",
    "As you can see, the calculations are based solely on the centroids from the allclusters variable, without any reference to the original data in F_dff_dec. So this code is usable for all runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99125b52-acc7-4c25-abfc-8923310eb6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all the centroids from the 'allclusters' variable\n",
    "num_clusters = allclusters_new.shape[0]\n",
    "centroids_from_data = [allclusters_new[i, 1] for i in range(num_clusters)]\n",
    "\n",
    "# Convert centroids list to a numpy array\n",
    "centroids_array = np.vstack(centroids_from_data) # vertically stacking the list of centroids to form a matrix where each row is a centroid\n",
    "\n",
    "# Compute the pairwise correlation coefficients between centroids\n",
    "centroid_correlations = np.corrcoef(centroids_array)\n",
    "\n",
    "# Create a new figure with a specific size\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Display the correlation matrix as an image\n",
    "cax = plt.imshow(centroid_correlations, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "\n",
    "# Add a color bar to the side\n",
    "cbar = plt.colorbar(cax, shrink=0.75)\n",
    "cbar.set_label('Correlation Coefficient', rotation=270, labelpad=20)\n",
    "\n",
    "# Annotate the heatmap with the correlation values\n",
    "for i in range(centroid_correlations.shape[0]):\n",
    "    for j in range(centroid_correlations.shape[1]):\n",
    "        plt.text(j, i, \"{:.2f}\".format(centroid_correlations[i, j]),\n",
    "                 ha='center', va='center', color='black' if abs(centroid_correlations[i, j]) < 0.5 else 'white')\n",
    "\n",
    "# Adjust the appearance of the plot\n",
    "plt.title(\"Pairwise Correlation Coefficients Between Cluster Centroids\")\n",
    "plt.xlabel(\"Cluster Centroid Index\")\n",
    "plt.ylabel(\"Cluster Centroid Index\")\n",
    "plt.xticks(np.arange(centroid_correlations.shape[0]), np.arange(1, centroid_correlations.shape[0] + 1))\n",
    "plt.yticks(np.arange(centroid_correlations.shape[1]), np.arange(1, centroid_correlations.shape[1] + 1))\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "if save_figure:\n",
    "    plt.savefig(os.path.join(path_clustering_data, 'Corrcoeffs-btw-Cluster-Centroids-new.png'))\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c049ba-b46c-49e6-ac53-9c6a5d51c479",
   "metadata": {},
   "source": [
    "## 3) Spatial correlations between cluster centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f694ef1e-8355-4939-b038-e63c529db32a",
   "metadata": {},
   "source": [
    "Correlations between Cluster Centroids: Here, you're looking at the average or representative activity pattern of each cluster (the centroid) and computing correlations based on these. This provides a higher-level view, focusing on the relationships between groups of neurons (clusters) rather than individual neurons.\n",
    "\n",
    "If you're  interested in understanding how clusters of neurons (groups of neurons with similar activity) are spatially organized with respect to each other, then using the correlations between cluster centroids would be the right approach.\n",
    "\n",
    "**********************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1e7257-b54e-45bc-b9ef-db7f1e802e04",
   "metadata": {},
   "source": [
    "Correlations between Individual Neurons' Activity Patterns (see section #4): This approach considers the activity patterns of every single neuron in the field of view. When you compute pairwise correlations based on these, you're assessing how similar each neuron's activity is to every other neuron's activity. This gives a very detailed view of the relationships between all neurons.\n",
    "\n",
    "For the specific goal of understanding how the mean correlation between neurons changes as a function of their spatial distance, using the pairwise correlations between individual neurons' activity patterns would be more appropriate. This is because you want to understand the spatial organization of individual neurons based on their activity patterns."
   ]
  },
  {
   "cell_type": "raw",
   "id": "c61f1445-191f-464a-a797-783e08659e3c",
   "metadata": {},
   "source": [
    "# 1) Compute the spatial coordinates for each cluster centroid\n",
    "\n",
    "# Initialize an empty list to store cluster centroids\n",
    "cluster_centroid_coords = []\n",
    "\n",
    "# Loop through to extract the centroids of neurons within each cluster and compute the mean of the coordinates\n",
    "for cluster in allclusters_new[:, 0]:\n",
    "    cluster_neurons_coords = filtered_centroids[cluster-1]  # Adjusting for 0-based index in Python\n",
    "    \n",
    "    # Reshape the cluster_neurons_coords if necessary\n",
    "    # (There is usually a problem related to the shape of the spatial coordinates data for certain clusters that gets corrected here)\n",
    "    if len(cluster_neurons_coords.shape) == 3 and cluster_neurons_coords.shape[0] == 1:\n",
    "        cluster_neurons_coords = cluster_neurons_coords.reshape(cluster_neurons_coords.shape[1], 2)\n",
    "    \n",
    "    if cluster_neurons_coords.size == 0 or len(cluster_neurons_coords.shape) != 2:\n",
    "        print(f\"Skipped cluster with indices {cluster} due to irregular shape\")\n",
    "    else:\n",
    "        cluster_centroid = cluster_neurons_coords.mean(axis=0)\n",
    "        cluster_centroid_coords.append(cluster_centroid)\n",
    "\n",
    "cluster_centroid_coords = np.array(cluster_centroid_coords)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ed2e90ac-bd86-441a-82d2-8de74f253b92",
   "metadata": {},
   "source": [
    "# 2) Compute pairwise spatial distances and extract the upper triangle distances\n",
    "pairwise_cluster_centroid_distances = squareform(pdist(cluster_centroid_coords))\n",
    "upper_triangle_indices = np.triu_indices_from(pairwise_cluster_centroid_distances, k=1)\n",
    "unique_pairwise_distances = pairwise_cluster_centroid_distances[upper_triangle_indices]\n",
    "\n",
    "flattened_correlations = centroid_correlations[upper_triangle_indices]\n",
    "\n",
    "# Calculate the IQR for the unique pairwise distances\n",
    "Q1 = np.percentile(unique_pairwise_distances, 25)\n",
    "Q3 = np.percentile(unique_pairwise_distances, 75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# 3) Calculate the number of data points and bin size using the Freedman-Diaconis rule (instead of doing it manually/arbitrarily)\n",
    "# F-D uses the interquartile range and the cube root of the number of data points to determine bin width.\n",
    "n = len(unique_pairwise_distances)\n",
    "bin_size = 2 * IQR / (n ** (1/3))\n",
    "bin_size = max(round(bin_size), 1) # round to nearest whole number\n",
    "# bin_size = 59 # if you want to manually input the bin_size\n",
    "\n",
    "# If the bin width is rounded down to 0, we should set it to at least 1 to avoid zero-width bins\n",
    "if bin_size == 0:\n",
    "    bin_size = 1\n",
    "\n",
    "# Calculate the total number of bins needed (i.e. the array of bin edges calculated from the bin_size)\n",
    "bins = np.arange(start=min(unique_pairwise_distances), \n",
    "                 stop=max(unique_pairwise_distances) + bin_size, \n",
    "                 step=bin_size)\n",
    "# The np.arange function creates an array that starts from 0 and goes up to the maximum value of pairwise_cluster_centroid_distances in increments of \"bin_size\" pixels. \n",
    "# So, each bin represents a range of X pixels: e.g. if bin_size = 10 then [0, 10), [10, 20), [20, 30), and so on.\n",
    "bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "# Loop through each bin and compute average correlations and standard deviations\n",
    "# i.e. Define a series of bins (e.g., 0-10 pixels, 10-20 pixels, etc.), and for each pair of cluster centroids that fall within \n",
    "# that bin, compute the average correlation using the centroid_correlations matrix.\n",
    "\n",
    "avg_correlations = []\n",
    "std_correlations = []\n",
    "\n",
    "for i in range(len(bins) - 1):\n",
    "    mask = (unique_pairwise_distances >= bins[i]) & (unique_pairwise_distances < bins[i+1])\n",
    "    avg_corr = np.mean(flattened_correlations[mask])\n",
    "    std_corr = np.std(flattened_correlations[mask])\n",
    "    avg_correlations.append(avg_corr)\n",
    "    std_correlations.append(std_corr)\n",
    "# This loop goes through each bin (except the last edge) and creates a mask that is True for distances within the current bin range. \n",
    "# It then computes the average and standard deviation of the centroid_correlations for distances that fall within that bin.\n",
    "\n",
    "# Filter out bins with NaN values in avg_correlations to ensure that you're only plotting bins with valid data points\n",
    "valid_bins = ~np.isnan(avg_correlations)\n",
    "bin_centers = bin_centers[valid_bins]\n",
    "avg_correlations = np.array(avg_correlations)[valid_bins]\n",
    "std_correlations = np.array(std_correlations)[valid_bins]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "71b148bf-1898-4f33-92bc-c65acb9f5cdf",
   "metadata": {},
   "source": [
    "# 4) Plot the average correlation for each distance bin (in micrometers)\n",
    "\n",
    "# Convert the bin_centers from pixels to micrometers\n",
    "bin_centers_micrometers = bin_centers * conversion_factor\n",
    "\n",
    "# 4) Plot the average correlation for each distance bin\n",
    "\n",
    "# This will give you a curve that shows how the average correlation between cluster centroids changes as a function of their spatial distance.\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.errorbar(bin_centers_micrometers, avg_correlations, yerr=std_correlations, fmt='-o', capsize=5)\n",
    "plt.xlabel('Spatial Distance Between Cluster Centroids (m)')\n",
    "plt.ylabel('Average Correlation')\n",
    "plt.title('Relationship between Spatial Distance and Average Correlation of Cluster Centroids \\n (using Freedman-Diaconis rule for binning)')\n",
    "plt.grid(True)\n",
    "\n",
    "# Annotate the bin size on the plot in the upper right corner\n",
    "# The location is dynamic and will adjust based on the axes dimensions because of 'axes fraction'\n",
    "bin_size_micrometers = bin_size * conversion_factor\n",
    "micrometer_label = \"m\" if bin_size_micrometers <= 1 else \"m\"\n",
    "\n",
    "# Use the correct form in the annotation\n",
    "plt.annotate(f'Bin Size: {round(bin_size_micrometers, 2)} {micrometer_label}', xy=(0.95, 0.95), xycoords='axes fraction', fontsize=10,\n",
    "             horizontalalignment='right', verticalalignment='top', bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"black\", lw=1))\n",
    "\n",
    "# Save the figure\n",
    "# if save_figure:\n",
    "    # plt.savefig(os.path.join(path_clustering_data, 'Spatial-Corr-btw-Cluster-Centroids-new.png'))\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d532361-ebde-4605-b106-78539168d12d",
   "metadata": {},
   "source": [
    "If clusters with similar activity patterns (i.e., high correlation) tend to be closer together, the curve will show higher correlations for shorter distances. Conversely, if similar activity patterns are distributed throughout the field of view, the curve will be flatter. This can help elucidate any spatial structure or organization in the neuronal activity of the observed area."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef24a6a-1dd7-450f-a456-7e1f7debdb69",
   "metadata": {},
   "source": [
    "## 4) Correlations between individual neurons' activity patterns\n",
    "Identifying whether neurons that are closer together tend to have more strongly correlated activity patterns.\n",
    "Plots show standard error bars NOT standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490d70c2-f565-41cd-825f-d4a14aa31ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin size is manually set to 150 micrometers\n",
    "\n",
    "# Load deconvolved fluorescence signal data depending on the condition being analyzed\n",
    "if run == 'run':\n",
    "    num_rows = matlab_data['F_dff_dec'].shape[0]  # Get the number of rows\n",
    "    neuronal_signal_data = matlab_data['F_dff_dec'][0:min(6600, num_rows), :] # takes only the first 5min 30sec of spont. activity, as standardized in our experimental scheme \n",
    "elif run in ['run_1', 'run_2', 'run_3']:\n",
    "    neuronal_signal_data = matlab_data['F_dff_dec_segmented']\n",
    "elif run in ['run_4', 'run_5']:\n",
    "    neuronal_signal_data = matlab_data['F_dff_dec_subset']\n",
    "else:\n",
    "    print(\"Invalid run specified\")\n",
    "    neuronal_signal_data = None  # or some default value\n",
    "\n",
    "# Calculate pairwise correlations\n",
    "num_neurons = neuronal_signal_data.shape[1]\n",
    "correlation_matrix = np.zeros((num_neurons, num_neurons))\n",
    "for i in range(num_neurons):\n",
    "    for j in range(i+1, num_neurons):\n",
    "        corr, _ = pearsonr(neuronal_signal_data[:, i], neuronal_signal_data[:, j])\n",
    "        correlation_matrix[i, j] = corr\n",
    "        correlation_matrix[j, i] = corr\n",
    "\n",
    "# Calculate pairwise Euclidean distances between neurons\n",
    "spatial_coordinates = labeled_neurons[:, :2]\n",
    "distances_matrix = squareform(pdist(spatial_coordinates)) * conversion_factor # converting distances to micrometers\n",
    "\n",
    "# Define a fixed bin size in micrometers\n",
    "bin_size_micrometers = 150  # Adjust as needed\n",
    "\n",
    "# Calculate the maximum number of bins needed based on the maximum distance\n",
    "max_distance = np.max(distances_matrix)\n",
    "num_bins = int(np.ceil(max_distance / bin_size_micrometers))\n",
    "\n",
    "# Create the bins\n",
    "distance_bins = np.arange(0, bin_size_micrometers * (num_bins + 1), bin_size_micrometers)\n",
    "\n",
    "distance_bin_indices = np.digitize(distances_matrix, distance_bins)\n",
    "\n",
    "mean_correlations_per_bin = []\n",
    "stderr_correlations_per_bin = []  # Use standard error (not standard deviation)\n",
    "\n",
    "for i in range(1, len(distance_bins)):\n",
    "    indices = np.where(distance_bin_indices == i)\n",
    "    mean_corr = np.mean(correlation_matrix[indices])\n",
    "    \n",
    "    # Number of observations in the bin\n",
    "    num_observations = len(correlation_matrix[indices])\n",
    "    \n",
    "    # Calculate standard deviation\n",
    "    std_dev_corr = np.std(correlation_matrix[indices])\n",
    "    \n",
    "    # Calculate standard error: standard deviation divided by the square root of the number of observations\n",
    "    if num_observations > 0:\n",
    "        stderr_corr = std_dev_corr / np.sqrt(num_observations)\n",
    "    else:\n",
    "        stderr_corr = np.nan  # Assign NaN if no observations in the bin\n",
    "\n",
    "    mean_correlations_per_bin.append(mean_corr)\n",
    "    stderr_correlations_per_bin.append(stderr_corr)\n",
    "\n",
    "# Plotting FULL signal\n",
    "bins_center = (distance_bins[:-1] + distance_bins[1:]) / 2 # The bin centers will now also be consistently spaced according to the fixed bin size\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(bins_center, mean_correlations_per_bin, yerr=stderr_correlations_per_bin, fmt='o', ecolor='darkgray', elinewidth=3, capsize=5, linestyle='-', linewidth=2)\n",
    "plt.title('Mean Correlation vs. Spatial Distance Between Neurons \\n (using equally spaced binning)')\n",
    "plt.xlabel('Distance (m)')\n",
    "plt.ylabel('Mean Correlation Coefficient')\n",
    "plt.grid(True)\n",
    "\n",
    "# Annotate bin size in micrometers\n",
    "bin_size_micrometers = (bins_center[1] - bins_center[0])\n",
    "bin_size_micrometers = max(round(bin_size_micrometers), 1) # round to nearest whole number\n",
    "micrometer_label = \"m\" if bin_size_micrometers <= 1 else \"m\"\n",
    "plt.annotate(f'Bin Size: {round(bin_size_micrometers, 2)} {micrometer_label}', xy=(0.95, 0.95), xycoords='axes fraction', fontsize=10,\n",
    "             horizontalalignment='right', verticalalignment='top', bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"black\", lw=1))\n",
    "\n",
    "# Save the figure\n",
    "if save_figure:\n",
    "    plt.savefig(os.path.join(path_clustering_data, 'FINAL-Full-Mean-Correlation-vs-Spatial-Distance-btw-Neurons.png'))\n",
    "    plt.savefig(os.path.join(path_clustering_data, 'FINAL-Full-Mean-Correlation-vs-Spatial-Distance-btw-Neurons.svg'), format='svg')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6966db4-d566-4cd3-a135-61a807408d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving FULL dataset\n",
    "if save_figure:\n",
    "    data_to_save = pd.DataFrame({\n",
    "        'Distance (micrometers)': bins_center,\n",
    "        'Mean Correlation Coefficient': mean_correlations_per_bin,\n",
    "        'Standard Error of Correlation': stderr_correlations_per_bin\n",
    "    })\n",
    "\n",
    "    # Specify a file path to save the data\n",
    "    data_file_path = os.path.join(path_clustering_data, 'full_correlation_vs_distance_data.csv')\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    data_to_save.to_csv(data_file_path, index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ab7a23ba-6cd8-43f1-ac3f-0291216a3950",
   "metadata": {},
   "source": [
    "# Just plotting until 1000 micrometers on the x-axis\n",
    "\n",
    "bins_center = (distance_bins[:-1] + distance_bins[1:]) / 2 # The bin centers will now also be consistently spaced according to the fixed bin size\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(bins_center, mean_correlations_per_bin, yerr=stderr_correlations_per_bin, fmt='o', ecolor='darkgray', elinewidth=3, capsize=5, linestyle='-', linewidth=2)\n",
    "plt.title('Mean Correlation vs. Spatial Distance Between Neurons \\n (using equally spaced binning)')\n",
    "plt.xlabel('Distance (m)')\n",
    "plt.ylabel('Mean Correlation Coefficient')\n",
    "plt.grid(True)\n",
    "\n",
    "# Set x-axis limits to extend to 1000 micrometers\n",
    "plt.xlim(0, 1000)\n",
    "\n",
    "# Annotate bin size in micrometers\n",
    "bin_size_micrometers = (bins_center[1] - bins_center[0])\n",
    "bin_size_micrometers = max(round(bin_size_micrometers), 1) # round to nearest whole number\n",
    "micrometer_label = \"m\" if bin_size_micrometers <= 1 else \"m\"\n",
    "plt.annotate(f'Bin Size: {round(bin_size_micrometers, 2)} {micrometer_label}', xy=(0.95, 0.95), xycoords='axes fraction', fontsize=10,\n",
    "             horizontalalignment='right', verticalalignment='top', bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"black\", lw=1))\n",
    "\n",
    "# Save the figure\n",
    "if save_figure:\n",
    "    plt.savefig(os.path.join(path_clustering_data, 'FINAL-Segmented-Mean-Correlation-vs-Spatial-Distance-btw-Neurons.png'))\n",
    "\n",
    "# Save the figure as SVG\n",
    "if save_figure:\n",
    "    plt.savefig(os.path.join(path_clustering_data, 'FINAL-Segmented-Mean-Correlation-vs-Spatial-Distance-btw-Neurons.svg'), format='svg')\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "# Saving SEGMENTED dataset from 0 to 1000 micrometers\n",
    "data_to_save = pd.DataFrame({\n",
    "    'Distance (micrometers)': bins_center,\n",
    "    'Mean Correlation Coefficient': mean_correlations_per_bin,\n",
    "    'Standard Error of Correlation': stderr_correlations_per_bin\n",
    "})\n",
    "\n",
    "# Filter the DataFrame to include only distances within 1 to 1000 micrometers\n",
    "filtered_data = data_to_save[(data_to_save['Distance (micrometers)'] >= 1) & (data_to_save['Distance (micrometers)'] <= 1000)]\n",
    "\n",
    "# Specify a file path to save the data\n",
    "data_file_path = os.path.join(path_clustering_data, '1000um_correlation_vs_distance_data.csv')\n",
    "\n",
    "# Save the filtered DataFrame to a CSV file\n",
    "filtered_data.to_csv(data_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4115c32-ba8f-4ba1-87ca-c40505904cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting until 1000 micrometers and \"filtering\" parameters so that we truly analyze from 0 to 1000 \n",
    "bins_center = (distance_bins[:-1] + distance_bins[1:]) / 2  # The bin centers\n",
    "\n",
    "# Filter data to include only distances up to 1000 micrometers\n",
    "filter_mask = bins_center <= 1000\n",
    "filtered_bins_center = bins_center[filter_mask]\n",
    "filtered_mean_correlations = np.array(mean_correlations_per_bin)[filter_mask]\n",
    "filtered_stderr_correlations = np.array(stderr_correlations_per_bin)[filter_mask]  # Apply the same filter to stderr\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(filtered_bins_center, filtered_mean_correlations, yerr=filtered_stderr_correlations, fmt='o', ecolor='darkgray', elinewidth=3, capsize=5, linestyle='-', linewidth=2)\n",
    "\n",
    "plt.title('Mean Correlation vs. Spatial Distance Between Neurons \\n (using equally spaced binning)')\n",
    "plt.xlabel('Distance (m)')\n",
    "plt.ylabel('Mean Correlation Coefficient')\n",
    "plt.grid(True)\n",
    "\n",
    "# Set x-axis limits to extend to 1000 micrometers\n",
    "plt.xlim(0, 1000)\n",
    "\n",
    "# Annotate bin size in micrometers\n",
    "bin_size_micrometers = (bins_center[1] - bins_center[0])\n",
    "bin_size_micrometers = max(round(bin_size_micrometers), 1) # round to nearest whole number\n",
    "micrometer_label = \"m\" if bin_size_micrometers <= 1 else \"m\"\n",
    "plt.annotate(f'Bin Size: {round(bin_size_micrometers, 2)} {micrometer_label}', xy=(0.95, 0.95), xycoords='axes fraction', fontsize=10,\n",
    "             horizontalalignment='right', verticalalignment='top', bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"black\", lw=1))\n",
    "\n",
    "# Save the figure\n",
    "if save_figure:\n",
    "    plt.savefig(os.path.join(path_clustering_data, 'FINAL-Segmented-Mean-Correlation-vs-Spatial-Distance-btw-Neurons.png'))\n",
    "    plt.savefig(os.path.join(path_clustering_data, 'FINAL-Segmented-Mean-Correlation-vs-Spatial-Distance-btw-Neurons.svg'), format='svg')\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c170ad49-484b-41cd-8dd9-780805da7ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving SEGMENTED dataset from 0 to 1000 micrometers\n",
    "if save_figure:\n",
    "    data_to_save = pd.DataFrame({\n",
    "        'Distance (micrometers)': filtered_bins_center,\n",
    "        'Mean Correlation Coefficient': filtered_mean_correlations,\n",
    "        'Standard Error of Correlation': filtered_stderr_correlations\n",
    "    })\n",
    "\n",
    "    # Specify a file path to save the data\n",
    "    data_file_path = os.path.join(path_clustering_data, '1000um_correlation_vs_distance_data.csv')\n",
    "\n",
    "    # Save the filtered DataFrame to a CSV file\n",
    "    data_to_save.to_csv(data_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb3fcb7-8c03-47b4-8b69-2c2fa36d1270",
   "metadata": {},
   "source": [
    "### Fitting a 2nd degree polynomial and saving its coefficients to later compare them across organoids/conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea442f7e-eddd-4d46-b029-117168944699",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_center = (distance_bins[:-1] + distance_bins[1:]) / 2  # The bin centers\n",
    "\n",
    "# Filter data to include only distances up to 1000 micrometers\n",
    "filter_mask = bins_center <= 1000\n",
    "filtered_bins_center = bins_center[filter_mask]\n",
    "filtered_mean_correlations = np.array(mean_correlations_per_bin)[filter_mask]\n",
    "filtered_stderr_correlations = np.array(stderr_correlations_per_bin)[filter_mask]  # Apply the same filter to stderr\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(filtered_bins_center, filtered_mean_correlations, yerr=filtered_stderr_correlations, fmt='o', ecolor='darkgray', elinewidth=3, capsize=5, linestyle='-', linewidth=2)\n",
    "\n",
    "# Choose the order of the polynomial (2 for quadratic, 3 for cubic)\n",
    "polynomial_order = 3\n",
    "\n",
    "# Fit the polynomial to the filtered data\n",
    "poly_coefficients = np.polyfit(filtered_bins_center, filtered_mean_correlations, polynomial_order)\n",
    "\n",
    "# Create a polynomial function from the coefficients\n",
    "poly_function = np.poly1d(poly_coefficients)\n",
    "\n",
    "# Generate a smooth set of x-values for plotting the polynomial curve\n",
    "smooth_x = np.linspace(0, 1000, 500)\n",
    "\n",
    "# Calculate the corresponding y-values using the polynomial function\n",
    "smooth_y = poly_function(smooth_x)\n",
    "\n",
    "# Determine the correct suffix for the polynomial order\n",
    "if polynomial_order == 2:\n",
    "    order_suffix = \"nd\"\n",
    "elif polynomial_order == 3:\n",
    "    order_suffix = \"rd\"\n",
    "else:\n",
    "    order_suffix = \"th\"  # Default for other cases\n",
    "\n",
    "# Plot the polynomial curve with dynamic label\n",
    "plt.plot(smooth_x, smooth_y, label=f'{polynomial_order}{order_suffix} order polynomial fit')\n",
    "\n",
    "plt.title('Fitting a Polynomial Curve to \\n Mean Correlation vs. Spatial Distance Between Neurons \\n (using equally spaced binning)')\n",
    "plt.xlabel('Distance (m)')\n",
    "plt.ylabel('Mean Correlation Coefficient')\n",
    "plt.grid(True)\n",
    "\n",
    "# Set x-axis limits to extend to 1000 micrometers\n",
    "plt.xlim(0, 1000)\n",
    "\n",
    "# Annotate bin size in micrometers\n",
    "bin_size_micrometers = (bins_center[1] - bins_center[0])\n",
    "bin_size_micrometers = max(round(bin_size_micrometers), 1) # round to nearest whole number\n",
    "micrometer_label = \"m\" if bin_size_micrometers <= 1 else \"m\"\n",
    "plt.annotate(f'Bin Size: {round(bin_size_micrometers, 2)} {micrometer_label}', xy=(0.95, 0.95), xycoords='axes fraction', fontsize=10,\n",
    "             horizontalalignment='right', verticalalignment='top', bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"black\", lw=1))\n",
    "\n",
    "# Customize legend to match the annotation style\n",
    "legend = plt.legend(loc='upper right', bbox_to_anchor=(0.965, 0.90),  # Adjust location as needed\n",
    "                    frameon=True, framealpha=1, edgecolor='black', facecolor='white')\n",
    "legend.get_frame().set_linewidth(1)\n",
    "\n",
    "# Save the figure\n",
    "if save_figure:\n",
    "    plt.savefig(os.path.join(path_clustering_data, 'FINAL-FittingPoly-Segmented-Mean-Correlation-vs-Spatial-Distance-btw-Neurons.png'))\n",
    "\n",
    "# Save the figure as SVG\n",
    "if save_figure:\n",
    "    plt.savefig(os.path.join(path_clustering_data, 'FINAL-FittingPoly-Segmented-Mean-Correlation-vs-Spatial-Distance-btw-Neurons.svg'), format='svg')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca88c209-b67b-440c-9105-ca84c0b34887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving polynomial coefficients of the fitted curve\n",
    "if save_figure:\n",
    "    # Create a DataFrame from the coefficients\n",
    "    coefficients_df = pd.DataFrame({\n",
    "        'Order': range(polynomial_order, -1, -1),  # Orders from highest to lowest\n",
    "        'Coefficient': poly_coefficients\n",
    "    })\n",
    "\n",
    "    # Specify a file path to save the data\n",
    "    data_file_path = os.path.join(path_clustering_data, 'polynomial_coefficients.csv')\n",
    "\n",
    "    # Save the filtered DataFrame to a CSV file\n",
    "    coefficients_df.to_csv(data_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8910a299-86a7-4246-9476-b4fd79ae944e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin size is set dynamically instead of manually\n",
    "\n",
    "# Load deconvolved fluorescence signal data depending on the condition being analyzed\n",
    "if run == 'run':\n",
    "    num_rows = matlab_data['F_dff_dec'].shape[0]  # Get the number of rows\n",
    "    neuronal_signal_data = matlab_data['F_dff_dec'][0:min(6600, num_rows), :] # takes only the first 5min 30sec of spont. activity, as standardized in our experimental scheme \n",
    "elif run in ['run_1', 'run_2', 'run_3']:\n",
    "    neuronal_signal_data = matlab_data['F_dff_dec_segmented']\n",
    "elif run in ['run_4', 'run_5']:\n",
    "    neuronal_signal_data = matlab_data['F_dff_dec_subset']\n",
    "else:\n",
    "    print(\"Invalid run specified\")\n",
    "    neuronal_signal_data = None  # or some default value\n",
    "\n",
    "# Calculate pairwise correlations\n",
    "num_neurons = neuronal_signal_data.shape[1]\n",
    "correlation_matrix = np.zeros((num_neurons, num_neurons))\n",
    "for i in range(num_neurons):\n",
    "    for j in range(i+1, num_neurons):\n",
    "        corr, _ = pearsonr(neuronal_signal_data[:, i], neuronal_signal_data[:, j])\n",
    "        correlation_matrix[i, j] = corr\n",
    "        correlation_matrix[j, i] = corr\n",
    "\n",
    "# Calculate pairwise Euclidean distances between neurons\n",
    "spatial_coordinates = labeled_neurons[:, :2]\n",
    "distances_matrix = squareform(pdist(spatial_coordinates)) * conversion_factor # converting distances to micrometers\n",
    "\n",
    "# Define distance bins and calculate mean correlations for each bin\n",
    "max_distance = np.max(distances_matrix)\n",
    "distance_bins = np.linspace(0, max_distance, 20)\n",
    "distance_bin_indices = np.digitize(distances_matrix, distance_bins)\n",
    "\n",
    "mean_correlations_per_bin = []\n",
    "stderr_correlations_per_bin = []  # Use standard error (not standard deviation)\n",
    "\n",
    "for i in range(1, len(distance_bins)):\n",
    "    indices = np.where(distance_bin_indices == i)\n",
    "    mean_corr = np.mean(correlation_matrix[indices])\n",
    "    \n",
    "    # Number of observations in the bin\n",
    "    num_observations = len(correlation_matrix[indices])\n",
    "    \n",
    "    # Calculate standard deviation\n",
    "    std_dev_corr = np.std(correlation_matrix[indices])\n",
    "    \n",
    "    # Calculate standard error: standard deviation divided by the square root of the number of observations\n",
    "    if num_observations > 0:\n",
    "        stderr_corr = std_dev_corr / np.sqrt(num_observations)\n",
    "    else:\n",
    "        stderr_corr = np.nan  # Assign NaN if no observations in the bin\n",
    "\n",
    "    mean_correlations_per_bin.append(mean_corr)\n",
    "    stderr_correlations_per_bin.append(stderr_corr)\n",
    "\n",
    "# Plotting\n",
    "bins_center = (distance_bins[:-1] + distance_bins[1:]) / 2\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(bins_center, mean_correlations_per_bin, yerr=stderr_correlations_per_bin, fmt='o', ecolor='darkgray', elinewidth=3, capsize=5, linestyle='-', linewidth=2)\n",
    "plt.title('Mean Correlation vs. Spatial Distance Between Neurons \\n (using equally spaced binning that is dynamically defined)')\n",
    "plt.xlabel('Distance (m)')\n",
    "plt.ylabel('Mean Correlation Coefficient')\n",
    "plt.grid(True)\n",
    "\n",
    "# Annotate bin size in micrometers\n",
    "bin_size_micrometers = (bins_center[1] - bins_center[0])\n",
    "bin_size_micrometers = max(round(bin_size_micrometers), 1) # round to nearest whole number\n",
    "micrometer_label = \"m\" if bin_size_micrometers <= 1 else \"m\"\n",
    "plt.annotate(f'Bin Size: {round(bin_size_micrometers, 2)} {micrometer_label}', xy=(0.95, 0.95), xycoords='axes fraction', fontsize=10,\n",
    "             horizontalalignment='right', verticalalignment='top', bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"black\", lw=1))\n",
    "\n",
    "# Save the figure\n",
    "if save_figure:\n",
    "    plt.savefig(os.path.join(path_clustering_data, 'Updated-DynamicBinning-Full-Mean-Correlation-vs-Spatial-Distance-btw-Neurons.png'))\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bad3c09c-2b5f-4edc-a8f4-b27f920938bd",
   "metadata": {},
   "source": [
    "# Just plotting until 1000 micrometers on the x-axis\n",
    "\n",
    "bins_center = (distance_bins[:-1] + distance_bins[1:]) / 2\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(bins_center, mean_correlations_per_bin, yerr=stderr_correlations_per_bin, fmt='o', ecolor='darkgray', elinewidth=3, capsize=5, linestyle='-', linewidth=2)\n",
    "plt.title('Mean Correlation vs. Spatial Distance Between Neurons \\n (using equally spaced binning that is dynamically defined)')\n",
    "plt.xlabel('Distance (m)')\n",
    "plt.ylabel('Mean Correlation Coefficient')\n",
    "plt.grid(True)\n",
    "\n",
    "# Set x-axis limits to extend to 500 micrometers\n",
    "plt.xlim(0, 1000)\n",
    "\n",
    "# Annotate bin size in micrometers\n",
    "bin_size_micrometers = (bins_center[1] - bins_center[0])\n",
    "bin_size_micrometers = max(round(bin_size_micrometers), 1) # round to nearest whole number\n",
    "micrometer_label = \"m\" if bin_size_micrometers <= 1 else \"m\"\n",
    "plt.annotate(f'Bin Size: {round(bin_size_micrometers, 2)} {micrometer_label}', xy=(0.95, 0.95), xycoords='axes fraction', fontsize=10,\n",
    "             horizontalalignment='right', verticalalignment='top', bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"black\", lw=1))\n",
    "\n",
    "# Save the figure\n",
    "# if save_figure:\n",
    "    # plt.savefig(os.path.join(path_clustering_data, 'Updated-DynamicBinning-Segmented-Mean-Correlation-vs-Spatial-Distance-btw-Neurons.png'))\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b412e1b-3927-447d-86a8-c922c83ba1b9",
   "metadata": {},
   "source": [
    "The analysis of how correlation coefficients vary with spatial distances between neurons has been completed. We categorized the distances into bins and calculated the mean correlation for each bin. The results show that the mean correlation tends to decrease as the distance between neurons increases, which is a common pattern in spatially structured data. This visual representation can help in understanding the spatial organization and connectivity of neurons based on their activity patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a61b840-3440-4d8d-9960-d26c642dbe6d",
   "metadata": {},
   "source": [
    "#### Identify the top 10 neuron pairs with the highest correlations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e102934-a2e7-4982-b397-ad98d5ad74d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the upper triangle of the correlation matrix to avoid duplicates\n",
    "upper_tri_corr = correlation_matrix[np.triu_indices_from(correlation_matrix, k=1)]\n",
    "\n",
    "# Find the indices of the top ten correlations\n",
    "top_indices = np.argsort(upper_tri_corr)[-10:][::-1]  # Sort and reverse to get top values\n",
    "\n",
    "# Extract the corresponding neuron pairs\n",
    "neuron_pairs_top_correlations = np.array(np.triu_indices_from(correlation_matrix, k=1)).T[top_indices]\n",
    "\n",
    "# Display information about these neuron pairs including cluster assignments\n",
    "for i, (neuron1, neuron2) in enumerate(neuron_pairs_top_correlations):\n",
    "    corr_value = correlation_matrix[neuron1, neuron2]\n",
    "    coord1, coord2 = spatial_coordinates[neuron1], spatial_coordinates[neuron2]\n",
    "    cluster1, cluster2 = labeled_neurons[neuron1, 2], labeled_neurons[neuron2, 2]\n",
    "    print(f\"Pair {i+1}: Neuron {neuron1} (Cluster {cluster1}) and Neuron {neuron2} (Cluster {cluster2})\")\n",
    "    print(f\"   Correlation: {corr_value}\")\n",
    "    print(f\"   Coordinates: Neuron {neuron1} - {coord1}, Neuron {neuron2} - {coord2}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d23afb5-29df-493f-b521-c82b4358ee26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for saving\n",
    "data_for_saving = []\n",
    "for i, (neuron1, neuron2) in enumerate(neuron_pairs_top_correlations):\n",
    "    corr_value = correlation_matrix[neuron1, neuron2]\n",
    "    coord1, coord2 = spatial_coordinates[neuron1], spatial_coordinates[neuron2]\n",
    "    cluster1, cluster2 = labeled_neurons[neuron1, 2], labeled_neurons[neuron2, 2]\n",
    "    data_for_saving.append([neuron1, neuron2, corr_value, cluster1, cluster2, *coord1, *coord2])\n",
    "\n",
    "# Convert to a DataFrame\n",
    "df = pd.DataFrame(data_for_saving, columns=['Neuron1', 'Neuron2', 'Correlation', 'Cluster1', 'Cluster2', 'X1', 'Y1', 'X2', 'Y2'])\n",
    "\n",
    "# Save to CSV if save_figure is True\n",
    "if save_figure:\n",
    "    csv_filename = os.path.join(path_clustering_data, 'top_correlations.csv')\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9035601-22d7-47e6-9062-b8e472a5b674",
   "metadata": {},
   "source": [
    "#### Spatial locations of neurons with top 10 correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63478728-32f3-40d0-8897-c5b8d90282c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Display the background image\n",
    "plt.imshow(Cn, cmap='gray')\n",
    "\n",
    "# Overlay the scatter plot for all neurons\n",
    "scatter = plt.scatter(spatial_coordinates[:, 0], spatial_coordinates[:, 1], \n",
    "                      c=labeled_neurons[:, 2], cmap=custom_colormap, \n",
    "                      s=100, alpha=0.6, edgecolor=edge_colors)\n",
    "\n",
    "# Plot the top five neuron pairs with the highest correlations\n",
    "for pair_index in neuron_pairs_top_correlations:\n",
    "    neuron1, neuron2 = pair_index[0], pair_index[1]\n",
    "    plt.plot([spatial_coordinates[neuron1, 0], spatial_coordinates[neuron2, 0]], \n",
    "             [spatial_coordinates[neuron1, 1], spatial_coordinates[neuron2, 1]], \n",
    "             'ro-', linewidth=2, markersize=5)  # Red lines for top neuron pairs\n",
    "\n",
    "# Create legend for clusters\n",
    "adjusted_labels = ['Not Assigned' if cluster == 0 else f'Cluster {int(cluster)}' for cluster in unique_clusters]\n",
    "legend_patches = [mpatches.Patch(color=custom_colormap(cluster / (len(unique_clusters) - 1 if cluster != 0 else 1)), label=label) \n",
    "                  for cluster, label in sorted(zip(unique_clusters, adjusted_labels))]\n",
    "\n",
    "# Add legend entry for top correlations using a line symbol\n",
    "top_corr_line = mlines.Line2D([], [], color='red', marker='o', markersize=5, label='Top Correlations', linestyle='-')\n",
    "legend_patches.append(top_corr_line)\n",
    "\n",
    "plt.legend(handles=legend_patches, loc='upper left')\n",
    "plt.title('Spatial Locations of Neurons with Top 10 Correlations')\n",
    "plt.xlabel('X Coordinate')\n",
    "plt.ylabel('Y Coordinate')\n",
    "\n",
    "# Save the figure\n",
    "if save_figure:\n",
    "    plt.savefig(os.path.join(path_clustering_data, 'Spatial-Locations-of-Neurons-with-Top-Correlations.png'))\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad2201b-e15c-4718-be4f-1299052528a6",
   "metadata": {},
   "source": [
    "#### Plot the neural activity signal (i.e. deconvolved fluorescence signal) of neurons with top 10 correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc18cf2-0052-4d2e-b96c-6f4b1fcd917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_rate = 20  # frames per second\n",
    "\n",
    "# Flatten the upper triangle of the correlation matrix to avoid duplicates\n",
    "upper_tri_corr = correlation_matrix[np.triu_indices_from(correlation_matrix, k=1)]\n",
    "\n",
    "# Find the indices of the top ten correlations\n",
    "top_indices = np.argsort(upper_tri_corr)[-10:][::-1]  # Sort and reverse to get top values\n",
    "\n",
    "# Extract the corresponding neuron pairs\n",
    "neuron_pairs_top_correlations = np.array(np.triu_indices_from(correlation_matrix, k=1)).T[top_indices]\n",
    "\n",
    "# Number of frames\n",
    "num_frames = neuronal_signal_data.shape[0]\n",
    "time_seconds = np.arange(num_frames) / frame_rate  # Convert frame numbers to time in seconds\n",
    "\n",
    "# Plotting the neuronal activity for these pairs\n",
    "fig, axes = plt.subplots(10, 2, figsize=(15, 20))  # 10 rows, 2 columns\n",
    "\n",
    "for i, (neuron1, neuron2) in enumerate(neuron_pairs_top_correlations):\n",
    "    # Plot neuron1 activity\n",
    "    axes[i, 0].plot(time_seconds, neuronal_signal_data[:, neuron1] * 100)  # Multiply by 100 for %F/F\n",
    "    axes[i, 0].set_title(f'Neuron {neuron1} Activity')\n",
    "\n",
    "    # Plot neuron2 activity\n",
    "    axes[i, 1].plot(time_seconds, neuronal_signal_data[:, neuron2] * 100)  # Multiply by 100 for %F/F\n",
    "    axes[i, 1].set_title(f'Neuron {neuron2} Activity')\n",
    "\n",
    "    # Consistently annotate all neuron pair labels\n",
    "    axes[i, 0].annotate(f'Neuron Pair #{i+1}', xy=(-0.10, 0.5), xycoords='axes fraction', \n",
    "                        fontsize=12, ha='right', va='center', rotation=90)\n",
    "\n",
    "# Set global labels for the entire figure\n",
    "fig.text(0.5, 0.01, 'Time (seconds)', ha='center', va='center', fontsize=20)\n",
    "fig.text(0.01, 0.5, '% F/F', ha='center', va='center', rotation='vertical', fontsize=20)\n",
    "\n",
    "# Add a main title for the overall plot\n",
    "fig.suptitle('Deconvolved Fluorescence Signal of the Top Ten Neuron Pairs', fontsize=23)\n",
    "\n",
    "# Adjust the layout to prevent overlap\n",
    "fig.tight_layout(rect=[0.03, 0.03, 0.97, 0.97])\n",
    "\n",
    "# Save the figure\n",
    "if save_figure:\n",
    "    plt.savefig(os.path.join(path_clustering_data, 'Deconvolved-Fluorescence-Signal-of-Top-Ten-Neuron-Pairs.png'))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2790ce-a52c-443e-a29a-8f426f360a96",
   "metadata": {},
   "source": [
    "## 5) Spectral analysis\n",
    "Explore the frequency domain of the signals to reveal any oscillatory patterns that might not be evident in the time domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b747d8-0e88-432f-9491-9d0509be8355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FULL (i.e. not segmented/subset) deconvolved fluorescence signal data, keeping in mind the recodring durations of each run\n",
    "# For spectral analysis, we are no longer using the F_dff_dec_segmented or F_dff_dec_subset signal \n",
    "# (i.e. the ones that contain only the post-stimulation windows or just a subset of the full dataset)\n",
    "\n",
    "def slice_data(data, max_row):\n",
    "    num_rows = data.shape[0]  # Get the number of rows\n",
    "    return data[0:max_row, :] if num_rows >= max_row else data\n",
    "\n",
    "if run == 'run':\n",
    "    neuronal_signal_data = slice_data(matlab_data['F_dff_dec'], 6600) # 5min30sec\n",
    "elif run in ['run_1', 'run_2', 'run_3']:\n",
    "    if run == 'run_1':\n",
    "        neuronal_signal_data = slice_data(matlab_data['F_dff_dec'], 6600)\n",
    "    else:\n",
    "        neuronal_signal_data = slice_data(matlab_data['F_dff_dec'], 12600) # 10min30sec\n",
    "elif run in ['run_4', 'run_5']:\n",
    "    neuronal_signal_data = slice_data(matlab_data['F_dff_dec'], 12600)\n",
    "else:\n",
    "    print(\"Invalid run specified\")\n",
    "    neuronal_signal_data = None  # or some default value\n",
    "\n",
    "# The slice_data function checks if the number of columns in the data is greater than or equal to the maximum column index specified (max_col). If so, it slices the data up to that column; otherwise, it returns the data as is.\n",
    "# This function is then used in your if-elif-else structure to handle the slicing based on the run value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a741eb87-cee3-4ce6-961a-f1ea5597bd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Deconvolved Fluorescence Signal of the Top Ten Neuron Pairs \n",
    "# (as opposed to the plot we did earlier, which was with the segmented or subset data)\n",
    "\n",
    "# Number of frames\n",
    "num_frames = neuronal_signal_data.shape[0]\n",
    "time_seconds = np.arange(num_frames) / frame_rate  # Convert frame numbers to time in seconds\n",
    "\n",
    "# Plotting the neuronal activity for these pairs\n",
    "fig, axes = plt.subplots(10, 2, figsize=(15, 20))  # 10 rows, 2 columns\n",
    "\n",
    "for i, (neuron1, neuron2) in enumerate(neuron_pairs_top_correlations):\n",
    "    # Plot neuron1 activity\n",
    "    axes[i, 0].plot(time_seconds, neuronal_signal_data[:, neuron1] * 100)  # Multiply by 100 for %F/F\n",
    "    axes[i, 0].set_title(f'Neuron {neuron1} Activity')\n",
    "\n",
    "    # Plot neuron2 activity\n",
    "    axes[i, 1].plot(time_seconds, neuronal_signal_data[:, neuron2] * 100)  # Multiply by 100 for %F/F\n",
    "    axes[i, 1].set_title(f'Neuron {neuron2} Activity')\n",
    "\n",
    "    # Consistently annotate all neuron pair labels\n",
    "    axes[i, 0].annotate(f'Neuron Pair #{i+1}', xy=(-0.10, 0.5), xycoords='axes fraction', \n",
    "                        fontsize=12, ha='right', va='center', rotation=90)\n",
    "\n",
    "# Set global labels for the entire figure\n",
    "fig.text(0.5, 0.01, 'Time (seconds)', ha='center', va='center', fontsize=20)\n",
    "fig.text(0.01, 0.5, '% F/F', ha='center', va='center', rotation='vertical', fontsize=20)\n",
    "\n",
    "# Add a main title for the overall plot\n",
    "fig.suptitle('Complete Deconvolved Fluorescence Signal of the Top Ten Neuron Pairs', fontsize=23)\n",
    "\n",
    "# Adjust the layout to prevent overlap\n",
    "fig.tight_layout(rect=[0.03, 0.03, 0.97, 0.97])\n",
    "\n",
    "# Save the figure\n",
    "if save_figure:\n",
    "    plt.savefig(os.path.join(path_clustering_data, 'Complete-Deconvolved-Fluorescence-Signal-of-Top-Ten-Neuron-Pairs.png'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1cfff8-565d-4a32-b4d8-0ae8a311336f",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuronal_signal_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a136a190-da27-4161-976d-25b655989b14",
   "metadata": {},
   "source": [
    "### Welch's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b6e1c4-1357-4c0e-af9f-a1d8aed39e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the Power Spectral Density (PSD) of a signal\n",
    "def plot_psd(signal, ax, title, fs=20):\n",
    "    \"\"\"\n",
    "    Calculate and plot the Power Spectral Density (PSD) of a signal on a given axes.\n",
    "    \n",
    "    :param signal: Input signal\n",
    "    :param ax: Axes to plot on\n",
    "    :param title: Title for the plot\n",
    "    :param fs: Sampling frequency\n",
    "    \"\"\"\n",
    "    freqs, psd = welch(signal, fs=fs)\n",
    "    ax.semilogy(freqs, psd)\n",
    "    ax.set_title(title)\n",
    "    # ax.set_xlabel('Frequency')\n",
    "    # ax.set_ylabel('Power')\n",
    "    ax.grid(True)\n",
    "\n",
    "# Spectral analysis for each neuron in the top pairs\n",
    "fig, axes = plt.subplots(10, 2, figsize=(15, 20))\n",
    "\n",
    "fs = 20  # Define the sampling frequency if known\n",
    "for i, (neuron1, neuron2) in enumerate(neuron_pairs_top_correlations):\n",
    "    signal1 = neuronal_signal_data[:, neuron1]\n",
    "    signal2 = neuronal_signal_data[:, neuron2]\n",
    "\n",
    "    plot_psd(signal1, axes[i, 0], f'Neuron {neuron1} PSD')\n",
    "    plot_psd(signal2, axes[i, 1], f'Neuron {neuron2} PSD')\n",
    "\n",
    "    # Consistently annotate all neuron pair labels\n",
    "    axes[i, 0].annotate(f'Neuron Pair #{i+1}', xy=(-0.10, 0.5), xycoords='axes fraction', \n",
    "                        fontsize=12, ha='right', va='center', rotation=90)\n",
    "\n",
    "# Set global labels for the entire figure\n",
    "fig.text(0.5, 0.01, 'Frequency (Hz)', ha='center', va='center', fontsize=20)\n",
    "fig.text(0.01, 0.5, 'Power (a.u./Hz)', ha='center', va='center', rotation='vertical', fontsize=20)\n",
    "\n",
    "# Adjust the layout to prevent overlap and add a main title for the overall plot\n",
    "fig.suptitle('Power Spectral Density (PSD) of Each Neuron in the Top Ten Pairs \\n (using Welch Method)', fontsize=23)\n",
    "fig.tight_layout(rect=[0.03, 0.03, 0.97, 0.97])\n",
    "\n",
    "# Save the figure\n",
    "if save_figure:\n",
    "    plt.savefig(os.path.join(path_clustering_data, 'Power-Spectral-Density-of-Each-Neuron-in-Top-Ten-Pairs.png'))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d568cd-fdf2-46b0-8540-843d1279b118",
   "metadata": {},
   "source": [
    "### Wavelet Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8212551-e385-46eb-909d-dad1b15e9107",
   "metadata": {},
   "source": [
    "Without specific parameters, \"cmor\" uses default values for its bandwidth and center frequency, which might not be optimal for all types of data. Therefore, I changed the wavelet_name parameter in the plot_cwt function to 'cmor1.5-1.0'. This specifies a Continuous Morlet Wavelet with a bandwidth frequency of 1.5 and a center frequency of 1.0. You can adjust these values (B and C in cmorB-C) to experiment with different wavelet properties. By specifying these parameters, you tailor the wavelet to better suit the specific characteristics of your data, like the frequency range of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c254c172-2e00-450b-b41d-b5e5df3c2dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale on y-axis: Often used for detailed analysis where the relationship between the wavelet scales and the signal is of interest.\n",
    "\n",
    "def plot_cwt(signal, ax, title, scales, wavelet_name='cmor1.5-1.0', frame_rate=20):\n",
    "    \"\"\"\n",
    "    Perform and plot the Continuous Wavelet Transform (CWT) of a signal.\n",
    "    \n",
    "    :param signal: Input signal\n",
    "    :param ax: Axes to plot on\n",
    "    :param title: Title for the plot\n",
    "    :param scales: Array of scales for the CWT\n",
    "    :param wavelet_name: Name of the wavelet to use\n",
    "    :param frame_rate: Frame rate for converting frame numbers to time\n",
    "    \"\"\"\n",
    "    coefficients, frequencies = pywt.cwt(signal, scales, wavelet_name)\n",
    "    # Convert frame numbers to time in seconds\n",
    "    time_seconds = np.arange(len(signal)) / frame_rate\n",
    "    ax.imshow(np.abs(coefficients), extent=[0, time_seconds[-1], 1, max(scales)], cmap='jet', aspect='auto')\n",
    "    ax.set_title(title)\n",
    "    # ax.set_xlabel('Time (seconds)')\n",
    "    # ax.set_ylabel('Scale (a.u.)')\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "# Define scales for the CWT\n",
    "scales = np.arange(1, 128)\n",
    "\n",
    "# Plot CWT for each neuron in the top pairs\n",
    "fig, axes = plt.subplots(10, 2, figsize=(15, 20))\n",
    "\n",
    "for i, (neuron1, neuron2) in enumerate(neuron_pairs_top_correlations):\n",
    "    signal1 = neuronal_signal_data[:, neuron1]\n",
    "    signal2 = neuronal_signal_data[:, neuron2]\n",
    "\n",
    "    plot_cwt(signal1, axes[i, 0], f'Neuron {neuron1} CWT', scales)\n",
    "    plot_cwt(signal2, axes[i, 1], f'Neuron {neuron2} CWT', scales)\n",
    "\n",
    "    axes[i, 0].annotate(f'Neuron Pair #{i+1}', xy=(-0.10, 0.5), xycoords='axes fraction', \n",
    "                        fontsize=12, ha='right', va='center', rotation=90)\n",
    "\n",
    "# Add global labels and main title\n",
    "fig.text(0.5, 0.01, 'Time (seconds)', ha='center', va='center', fontsize=20)\n",
    "fig.text(0.01, 0.5, 'Scale (a.u.)', ha='center', va='center', rotation='vertical', fontsize=20)\n",
    "fig.suptitle('Continuous Wavelet Transform (CWT) of Each Neuron in the Top Ten Pairs', fontsize=23)\n",
    "\n",
    "fig.tight_layout(rect=[0.03, 0.03, 0.97, 0.97])\n",
    "\n",
    "# Save the figure\n",
    "if save_figure:\n",
    "    plt.savefig(os.path.join(path_clustering_data, 'Scale-Continuous-Wavelet-Transform-of-Each-Neuron-in-Top-Ten-Pairs.png'))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17eefcc9-945c-4507-902e-2b5c8a027dd6",
   "metadata": {},
   "source": [
    "#### Frequency intead of Scale on the Y-Axis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fb5e7f-dd03-43b2-814f-f558a9bf0ded",
   "metadata": {},
   "source": [
    "In the original CWT implementation, the y-axis represented the scale, which is a standard way of depicting wavelet transform results. However, interpreting scales can be less intuitive because they are inversely related to frequency (i.e., a higher scale corresponds to a lower frequency and vice versa). Adjusting the y-axis to display frequency instead of scale makes the interpretation more straightforward:\n",
    "\n",
    "1) Direct Frequency Representation:\n",
    "Displaying frequency directly on the y-axis aligns with common expectations and interpretations, especially for those familiar with spectral analysis. It's easier to relate the findings to known frequency bands (like alpha, beta, etc., in neural data).\n",
    "\n",
    "2) Conversion from Scale to Frequency:\n",
    "The conversion from scale to frequency is done based on the properties of the chosen wavelet. For the Morlet wavelet, this conversion takes into account its center frequency and bandwidth to map scales to corresponding frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9e1556-9377-4875-8b82-03934f03a29f",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "Important to know:\n",
    "\n",
    "1. Frequency Range Analysis:\n",
    "\n",
    "The maximum frequency you can analyze is dictated by the Nyquist frequency, which is half of the sampling frequency. For a sampling rate of 20 Hz, the Nyquist frequency is 10 Hz. This means you can analyze frequencies up to 10 Hz. Any frequency component above 10 Hz cannot be reliably detected due to the sampling rate, and attempting to analyze these frequencies may lead to aliasing.\n",
    "\n",
    "2. Choice of Scales:\n",
    "\n",
    "The scales in the CWT should be chosen to cover the frequency range up to the Nyquist frequency. Since the data is sampled at 20 Hz, and neural activity of interest often occurs in lower frequency bands, you should select scales that correspond to the frequency range of 0-10 Hz.\n",
    "\n",
    "3. Wavelet Parameters:\n",
    "\n",
    "The wavelet's center frequency and bandwidth should be chosen considering the sampling rate to ensure the wavelet can capture the frequency components present in the data. For a 20 Hz sampling rate, a wavelet that focuses on lower frequencies (like the Morlet wavelet) would be suitable.\n",
    "\n",
    "4. Temporal Resolution:\n",
    "\n",
    "The sampling frequency also affects the temporal resolution of the analysis. With a 20 Hz sampling rate, each frame represents a 50ms interval. The CWT analysis will have this temporal granularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ea49f3-63b5-46b5-b127-699c8a82013e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency on y-axis: More commonly used for spectral analysis where the focus is on identifying the frequency components of the signal over time.\n",
    "\n",
    "def plot_cwt(signal, ax, title, fs, wavelet_name='cmor1.5-1.0'):\n",
    "    \"\"\"\n",
    "    Perform and plot the Continuous Wavelet Transform (CWT) of a signal.\n",
    "    \n",
    "    :param signal: Input signal\n",
    "    :param ax: Axes to plot on\n",
    "    :param title: Title for the plot\n",
    "    :param fs: Sampling frequency\n",
    "    :param wavelet_name: Name of the wavelet to use\n",
    "    \"\"\"\n",
    "    # Define the frequency range for CWT\n",
    "    max_freq = fs / 2\n",
    "    min_freq = 0.1  # Adjust as needed for your data\n",
    "    freqs = np.linspace(min_freq, max_freq, num=100)\n",
    "    \n",
    "    # Convert to scales\n",
    "    scales = pywt.scale2frequency(wavelet_name, [1 / freq for freq in freqs]) * fs\n",
    "    \n",
    "    # Perform CWT\n",
    "    coefficients, frequencies = pywt.cwt(signal, scales, wavelet_name, sampling_period=1/fs)\n",
    "    \n",
    "    # Convert frame numbers to time in seconds\n",
    "    time_seconds = np.arange(len(signal)) / fs\n",
    "    ax.imshow(np.abs(coefficients), extent=[0, time_seconds[-1], min_freq, max_freq], cmap='jet', aspect='auto', interpolation='nearest')\n",
    "    ax.set_title(title)\n",
    "    # ax.set_xlabel('Time (seconds)')\n",
    "    # ax.set_ylabel('Frequency (Hz)')\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    return coefficients, frequencies\n",
    "\n",
    "\n",
    "fs = 20  # Sampling frequency\n",
    "\n",
    "# Plot CWT for each neuron in the top pairs\n",
    "fig, axes = plt.subplots(10, 2, figsize=(15, 20))\n",
    "\n",
    "cwt_top10 = {}  # Dictionary to store coefficients to save them later\n",
    "\n",
    "for i, (neuron1, neuron2) in enumerate(neuron_pairs_top_correlations):\n",
    "    signal1 = neuronal_signal_data[:, neuron1]\n",
    "    signal2 = neuronal_signal_data[:, neuron2]\n",
    "\n",
    "    coefficients1, frequencies1 = plot_cwt(signal1, axes[i, 0], f'Neuron {neuron1} CWT', fs)\n",
    "    coefficients2, frequencies2 = plot_cwt(signal2, axes[i, 1], f'Neuron {neuron2} CWT', fs)\n",
    "\n",
    "    # Store the coefficients in the dictionary\n",
    "    cwt_top10[f'Neuron_{neuron1}'] = (coefficients1, frequencies1)\n",
    "    cwt_top10[f'Neuron_{neuron2}'] = (coefficients2, frequencies2)\n",
    "\n",
    "    axes[i, 0].annotate(f'Neuron Pair #{i+1}', xy=(-0.10, 0.5), xycoords='axes fraction', \n",
    "                        fontsize=12, ha='right', va='center', rotation=90)\n",
    "\n",
    "# Add global labels and main title\n",
    "fig.text(0.5, 0.01, 'Time (seconds)', ha='center', va='center', fontsize=20)\n",
    "fig.text(0.01, 0.5, 'Frequency (Hz)', ha='center', va='center', rotation='vertical', fontsize=20)\n",
    "fig.suptitle('Continuous Wavelet Transform (CWT) of Each Neuron in the Top Ten Pairs', fontsize=23)\n",
    "\n",
    "fig.tight_layout(rect=[0.03, 0.03, 0.97, 0.97])\n",
    "\n",
    "# Save the figure\n",
    "if save_figure:\n",
    "    plt.savefig(os.path.join(path_clustering_data, 'Freq-Continuous-Wavelet-Transform-of-Each-Neuron-in-Top-Ten-Pairs.png'))\n",
    "    plt.savefig(os.path.join(path_clustering_data, 'Freq-Continuous-Wavelet-Transform-of-Each-Neuron-in-Top-Ten-Pairs.svg'), format='svg')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Save cwt_top10\n",
    "if save_figure:\n",
    "    data_file_path = os.path.join(path_clustering_data, 'cwt_top10.npy')\n",
    "    np.save(data_file_path, cwt_top10)\n",
    "\n",
    "# Save as CSV\n",
    "if save_figure:\n",
    "    for neuron, (coefficients, frequencies) in cwt_top10.items():\n",
    "        # Create a DataFrame\n",
    "        df = pd.DataFrame(coefficients, index=frequencies, columns=np.arange(coefficients.shape[1]) / fs)\n",
    "        # Save to CSV\n",
    "        csv_file_path = os.path.join(path_clustering_data, f'{neuron}_cwt.csv')\n",
    "        df.to_csv(csv_file_path)\n",
    "\n",
    "# Each CSV file corresponds to one neuron's CWT coefficients.\n",
    "# The rows in these CSV files represent different frequencies (contained in the frequencies array).\n",
    "# The columns correspond to different time points (frame numbers converted to seconds)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a33ab0e-db84-4818-914c-c3681eb08545",
   "metadata": {},
   "source": [
    "### Continuous Wavelet Transform (CWT) analysis on entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ada4489-ecec-4ae1-90af-7b7ebb42f1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_cwt(signal, scales, wavelet_name='cmor1.5-1.0', fs=20):\n",
    "    \"\"\"\n",
    "    Perform the Continuous Wavelet Transform (CWT) on a given signal.\n",
    "    \n",
    "    :param signal: The input signal (one-dimensional array).\n",
    "    :param scales: Scales for the CWT.\n",
    "    :param wavelet_name: Name of the wavelet to use.\n",
    "    :param fs: Sampling frequency.\n",
    "    :return: CWT coefficients and corresponding frequencies.\n",
    "    \"\"\"\n",
    "    coefficients, frequencies = pywt.cwt(signal, scales, wavelet_name, sampling_period=1/fs)\n",
    "    return coefficients, frequencies\n",
    "\n",
    "\n",
    "num_neurons = neuronal_signal_data.shape[1]\n",
    "fs = 20  # Sampling frequency\n",
    "\n",
    "# Define frequency range for CWT\n",
    "max_freq = fs / 2\n",
    "min_freq = 0.1  # You can adjust this based on your data\n",
    "freqs = np.linspace(min_freq, max_freq, num=100)\n",
    "\n",
    "# Convert frequencies to scales\n",
    "scales = pywt.scale2frequency('cmor1.5-1.0', [1 / freq for freq in freqs]) * fs\n",
    "\n",
    "# Dictionary to store CWT results\n",
    "cwt_allresults = {}\n",
    "\n",
    "for neuron in range(num_neurons):\n",
    "    signal = neuronal_signal_data[:, neuron]\n",
    "    coeffs, freqs = perform_cwt(signal, scales, fs=fs)\n",
    "    cwt_allresults[neuron] = {\n",
    "        'coefficients': coeffs,\n",
    "        'frequencies': freqs\n",
    "    }\n",
    "\n",
    "# Save CWT results\n",
    "if save_figure:\n",
    "    data_file_path = os.path.join(path_clustering_data, 'cwt_allresults.npy')\n",
    "    np.save(data_file_path, cwt_allresults)\n",
    "\n",
    "# Save ALL RESULTS as CSV (each neuron's results are saved as a separate file)\n",
    "# if save_figure:\n",
    "    # for neuron, data in cwt_allresults.items():\n",
    "        # Extract coefficients and frequencies\n",
    "        # coeffs = data['coefficients']\n",
    "        # freqs = data['frequencies']\n",
    "\n",
    "        # Create a DataFrame\n",
    "        # Assuming coeffs is a 2D array (frequencies x time)\n",
    "        # and freqs is a 1D array (frequencies)\n",
    "        # df = pd.DataFrame(coeffs, index=freqs, columns=np.arange(coeffs.shape[1]) / fs)\n",
    "\n",
    "        # Save to CSV\n",
    "        # csv_file_path = os.path.join(path_clustering_data, f'neuron_{neuron}_cwt.csv')\n",
    "        # df.to_csv(csv_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18570759-7db4-45f9-91cc-50e5f91b56f4",
   "metadata": {},
   "source": [
    "#### Average CWT Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ab5468-3402-486a-825d-75e704451c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_rate = 20  # frames per second\n",
    "\n",
    "# Assuming cwt_allresults contains the CWT coefficients for each neuron\n",
    "cwt_avg = np.mean([np.abs(cwt_allresults[neuron]['coefficients']) for neuron in cwt_allresults], axis=0)\n",
    "\n",
    "# Convert frame numbers to time in seconds\n",
    "num_frames = cwt_avg.shape[1]\n",
    "time_seconds = np.arange(num_frames) / frame_rate\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(cwt_avg, aspect='auto', extent=[0, time_seconds[-1], min_freq, max_freq], cmap='jet')\n",
    "plt.title('Average CWT Coefficients Across All Neurons')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Frequency (Hz)')\n",
    "plt.colorbar(label='Magnitude')\n",
    "\n",
    "# Save the figure\n",
    "if save_figure:\n",
    "    plt.savefig(os.path.join(path_clustering_data, 'Average-Continuous-Wavelet-Transform-Coefficients-Across-All-Neurons.png'))\n",
    "    plt.savefig(os.path.join(path_clustering_data, 'Average-Continuous-Wavelet-Transform-Coefficients-Across-All-Neurons.svg'), format='svg')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Save cwt_avg\n",
    "if save_figure:\n",
    "    data_file_path = os.path.join(path_clustering_data, 'cwt_avg.npy')\n",
    "    np.save(data_file_path, cwt_avg)\n",
    "\n",
    "# Save as CSV\n",
    "if save_figure:\n",
    "    # Assuming freqs contains the frequencies corresponding to the rows of cwt_avg\n",
    "    df_cwt_avg = pd.DataFrame(cwt_avg, index=freqs, columns=time_seconds)\n",
    "    # Specify the path for the CSV file\n",
    "    csv_file_path = os.path.join(path_clustering_data, 'cwt_avg.csv')\n",
    "    # Save to CSV\n",
    "    df_cwt_avg.to_csv(csv_file_path)\n",
    "    \n",
    "# The rows of the CSV file correspond to different frequencies (contained in the freqs array).\n",
    "# The columns represent different time points, calculated as frame numbers converted to seconds (time_seconds array).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3f1734-ddbf-459f-bcec-6fc4634f0190",
   "metadata": {},
   "source": [
    "### 3D representations of CWT data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f95bc4-a383-48ca-a920-fa3c973f52c1",
   "metadata": {},
   "source": [
    "#### For average CWT coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb52c0bb-6074-4dfc-8491-c5cdf31542c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming cwt_allresults contains the CWT coefficients for each neuron\n",
    "cwt_avg = np.mean([np.abs(cwt_allresults[neuron]['coefficients']) for neuron in cwt_allresults], axis=0)\n",
    "\n",
    "# Prepare the time-frequency grid\n",
    "fs = 20  # Sampling frequency\n",
    "n_timepoints = cwt_avg.shape[1]\n",
    "time = np.linspace(0, n_timepoints / fs, n_timepoints)\n",
    "max_freq = fs / 2\n",
    "min_freq = 0.1  # Adjust as needed\n",
    "freqs = np.linspace(min_freq, max_freq, num=cwt_avg.shape[0])\n",
    "T, F = np.meshgrid(time, freqs)\n",
    "\n",
    "# Create a 3D plot\n",
    "fig = plt.figure(figsize=(15, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plotting the surface\n",
    "surf = ax.plot_surface(T, F, cwt_avg, cmap='jet', edgecolor='none')\n",
    "# viridis or jet\n",
    "\n",
    "# Labels and titles\n",
    "# ax.zaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "ax.set_title('Average CWT Coefficients Across All Neurons')\n",
    "ax.set_xlabel('Time (seconds)')\n",
    "ax.set_ylabel('Frequency (Hz)')\n",
    "ax.set_zlabel('Magnitude')\n",
    "\n",
    "# Adding a color bar with adjusted padding\n",
    "cbar = fig.colorbar(surf, ax=ax, shrink=0.5, aspect=8, pad=0.05)  # Increase pad to move colorbar to the right\n",
    "\n",
    "# Adjust the z-axis label position\n",
    "ax.set_zlabel('Magnitude', labelpad=8)  # Increase the labelpad value as needed\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(left=0.05, right=0.85)  # Adjust these parameters\n",
    "\n",
    "# Save the figure with bbox_inches='tight' to reduce white space\n",
    "if save_figure:\n",
    "    plt.savefig(os.path.join(path_clustering_data, 'Opt1-3D-Continuous-Wavelet-Transform-Coefficients-Average.png'), bbox_inches='tight')\n",
    "    plt.savefig(os.path.join(path_clustering_data, 'Opt1-3D-Continuous-Wavelet-Transform-Coefficients-Average.svg'), format='svg', bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4278337f-9e3f-46ae-84e0-30ecae127ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line Plots on a 3D Surface\n",
    "# Instead of using a filled surface, you can plot lines on a 3D graph. This approach uses the same time-frequency grid but plots lines for each frequency.\n",
    "\n",
    "fig = plt.figure(figsize=(15, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plotting lines for each frequency\n",
    "for i, freq in enumerate(freqs):\n",
    "    ax.plot(time, [freq] * len(time), cwt_avg[i, :], color=plt.cm.jet(i / len(freqs)))\n",
    "\n",
    "# Labels and titles\n",
    "# ax.zaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "ax.set_title('Average CWT Coefficients Across All Neurons')\n",
    "ax.set_xlabel('Time (seconds)')\n",
    "ax.set_ylabel('Frequency (Hz)')\n",
    "ax.set_zlabel('Magnitude')\n",
    "\n",
    "# Adding a color bar with adjusted padding\n",
    "cbar = fig.colorbar(surf, ax=ax, shrink=0.5, aspect=8, pad=0.05)  # Increase pad to move colorbar to the right\n",
    "\n",
    "# Adjust the z-axis label position\n",
    "ax.set_zlabel('Magnitude', labelpad=8)  # Increase the labelpad value as needed\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(left=0.05, right=0.85)  # Adjust these parameters\n",
    "\n",
    "# Save the figure with bbox_inches='tight' to reduce white space\n",
    "if save_figure:\n",
    "    plt.savefig(os.path.join(path_clustering_data, 'Opt2-3D-Continuous-Wavelet-Transform-Coefficients-Average.png'), bbox_inches='tight')\n",
    "    plt.savefig(os.path.join(path_clustering_data, 'Opt2-3D-Continuous-Wavelet-Transform-Coefficients-Average.svg'), format='svg', bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d9d242-ccff-4de0-b6b7-1e1dbd3af9f9",
   "metadata": {},
   "source": [
    "#### For top 10 neuron pairs with highest correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4777947c-a7e6-4e26-839e-087d009e72f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define frequency range and scales for CWT (common for all neurons)\n",
    "max_freq = fs / 2\n",
    "min_freq = 0.1  # Adjust as needed\n",
    "freqs = np.linspace(min_freq, max_freq, num=100)\n",
    "scales = pywt.scale2frequency('cmor1.5-1.0', [1 / freq for freq in freqs]) * fs\n",
    "\n",
    "n_pairs = 10\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig = plt.figure(figsize=(12, n_pairs * 8))\n",
    "\n",
    "for i, (neuron1, neuron2) in enumerate(neuron_pairs_top_correlations):\n",
    "    # Prepare the time-frequency grid\n",
    "    n_timepoints = neuronal_signal_data.shape[0]\n",
    "    time = np.linspace(0, n_timepoints / fs, n_timepoints)\n",
    "    T, F = np.meshgrid(time, freqs)\n",
    "    \n",
    "    # Perform CWT for neuron1 and neuron2\n",
    "    coefficients1, _ = pywt.cwt(neuronal_signal_data[:, neuron1], scales, 'cmor1.5-1.0', sampling_period=1/fs)\n",
    "    coefficients2, _ = pywt.cwt(neuronal_signal_data[:, neuron2], scales, 'cmor1.5-1.0', sampling_period=1/fs)\n",
    "\n",
    "    # Plot for neuron1\n",
    "    ax1 = fig.add_subplot(n_pairs, 2, 2*i+1, projection='3d')\n",
    "    # ax1.xaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "    # ax1.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "    ax1.zaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "    ax1.plot_surface(T, F, np.abs(coefficients1), cmap='jet', edgecolor='none')\n",
    "    ax1.set_title(f'Neuron {neuron1} CWT', pad=10)  # Adjust title spacing with the pad parameter\n",
    "    ax1.set_xlabel('Time (s)')\n",
    "    ax1.set_ylabel('Frequency (Hz)')\n",
    "    ax1.set_zlabel('Magnitude')\n",
    "    ax1.text2D(-0.15, 0.5, f'Neuron Pair #{i+1}', transform=ax1.transAxes, fontsize=12, ha='center', va='center', rotation=90)\n",
    "\n",
    "    # Plot for neuron2\n",
    "    ax2 = fig.add_subplot(n_pairs, 2, 2*i+2, projection='3d')\n",
    "    # ax2.xaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "    # ax2.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "    ax2.zaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "    ax2.plot_surface(T, F, np.abs(coefficients2), cmap='jet', edgecolor='none')\n",
    "    ax2.set_title(f'Neuron {neuron2} CWT', pad=10)  # Adjust title spacing with the pad parameter\n",
    "    ax2.set_xlabel('Time (s)')\n",
    "    ax2.set_ylabel('Frequency (Hz)')\n",
    "    ax2.set_zlabel('Magnitude')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=1)  # Reduce the top margin to make space for the title\n",
    "\n",
    "# Add a main title for the overall plot, adjust y position\n",
    "# fig.suptitle('CWT Coefficients of Each Neuron in the Top Ten Pairs', fontsize=23, y=1)\n",
    "\n",
    "# Adjust the spacing\n",
    "plt.subplots_adjust(wspace=0.4, hspace=0.4, right=0.9)  # Adjust 'right' parameter as needed\n",
    "\n",
    "# Save the figure\n",
    "if save_figure:\n",
    "    plt.savefig(os.path.join(path_clustering_data, '3D-Continuous-Wavelet-Transform-Coefficients-Top-10-Neurons.png'))\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
